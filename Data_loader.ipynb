{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9ju2L+v8SfKcOt0I6/cw+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav-Tyro/keyphrase-extraction-using-BERT/blob/main/Data_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri8MKP1Zl1Oh",
        "outputId": "611b7bbd-f161-4174-b12b-678e68cc2b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "\"\"\" importing all the libraries for data loading\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "import utils\n",
        "class DataLoader(object):\n",
        "  def__init(self, data_dir, bert_model_dir, params, token_pad_idx=0): \n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = params.batch_size\n",
        "    self.max_len = params.max_len\n",
        "    self.device = params.device\n",
        "    self.seed = params.seed\n",
        "    self.token_pad_idx = 0\n",
        "\n",
        "    tags=self.load_tags()\n",
        "    self.tag2idx = {tag : idx for idx,tag in enumerate(tags)}\n",
        "    self.idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
        "    params.tag2idx = self.tag2idx \n",
        "    params.idx2tag = self.idx2tag\n",
        "    self.tag_pad_idx = self.tag2idx['0']\n",
        "\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(bert_model_dir, do_lower_case = True)\n",
        "\n",
        "    def load_tags(self):\n",
        "      tags = []\n",
        "      file_path = os.path.join(self.data_dir, 'tags.txt')\n",
        "      with open(file_path, 'r') as file:\n",
        "        for tag in file:\n",
        "          tags.append(tags.strip())\n",
        "      return tags\n",
        "\n",
        "      def load_sentence_tags(self, sentences_file, tags_file, d):\n",
        "        \"\"\" loads sentences and tags from their corresponding files.\n",
        "            maps  tokens and tags from their indicies and stores them in the provided dict d.\n",
        "        \"\"\"\n",
        "        sentence = []\n",
        "        tags = []\n",
        "\n",
        "        with open(sentence_file, 'r') as file:\n",
        "          for line in file:\n",
        "            # replace each tag by it's index\n",
        "            tokens = line.split()\n",
        "            sentences.append(self.tokenizer.convert_tokens_to_ids(tokens))\n",
        "\n",
        "        with open(tags_file, 'r') as file:\n",
        "          for line in file:\n",
        "            #replace each tag by it's index\n",
        "             tag_seq = [self.tag2idx.get(tag) for tag in line.strip().split(' ')]\n",
        "             tags.append(tag_seq)\n",
        "\n",
        "        # checks to ensure there is a tag for each token \n",
        "        assert len(sentence) == len(tags)\n",
        "        for i in range(len(sentences)):\n",
        "            # print(sentences[i], tags[i])\n",
        "          assert len(tags[i]) == len(sentences[i])\n",
        "\n",
        "        # storing sentences and tags in dict d\n",
        "        d['data'] = sentences\n",
        "        d['tags'] = tags\n",
        "        d['size'] = len(sentences)\n",
        "    def load_data(self, data_type):\n",
        "      \"\"\" loads the data for each type in types from data_dir.\n",
        "\n",
        "      Args:\n",
        "          data_type: (str) has one of 'train', 'val', 'test' depending on which data is required.\n",
        "      Returns:\n",
        "          data: (dict) contains the data with tags for each type in types.\n",
        "      \"\"\"\n",
        "      data = {}\n",
        "\n",
        "      if data_type in ['train', 'val', 'test']:\n",
        "        sentence_file = os.path.join(self.data_dir, data_type, 'sentence.txt')\n",
        "        tags_path = os.path.join(self.data_dir, data_type, 'tags.txt')   \n",
        "        self.load_sentence_tags(sentences_file, tags_path, data)\n",
        "      else:\n",
        "          raise ValueError(\"data type not in ['train', 'val', 'test']\")\n",
        "      return data\n",
        "\n",
        "    def data_iterator(self,data,shuffle=false):\n",
        "      \"\"\"Returns a generator that yields betches data with tags.\n",
        "\n",
        "      Args:\n",
        "          data: (dict) contain data which has keys 'data', 'tags' and 'size'\n",
        "          shuffle: (bool) whether the data should be shuffled\n",
        "\n",
        "      yields:\n",
        "          batch_data: (tensor) shape: (batch_size, max_len)\n",
        "          batch_tags: (tensor) shape: (batch_size, max_len)\n",
        "      \"\"\"\n",
        "\n",
        "      # make a list that decides the order in which we go over the data-this avoids explicit shuffling of data\n",
        "      order = list(range(data['size']))\n",
        "      if shuffle:\n",
        "        random.seed(self.seed)\n",
        "        random.shuffle(order)\n",
        "      \n",
        "      # one pass over data\n",
        "      for i in range(data['size']//self.batch_size):\n",
        "        # fetch sentences and tags\n",
        "        sentences = [data['data'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
        "        tags = [data['tags'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
        "\n",
        "        # batch length\n",
        "        batch_len = len(sentences)\n",
        "\n",
        "        # compute length of longest sentence in batch\n",
        "        batch_max_len = max([len(s) for s in sentences])\n",
        "        max_len = min(batch_max_len, self.max_len)\n",
        "\n",
        "        # prepare a numpy array with the data, initialising the data with pad_idx\n",
        "        batch_data = self.token_pad_idx*np.ones((batch_len, max_len))\n",
        "        batch_tags = self.tag_pad_idx * np.ones((batch_len, max_len))\n",
        "\n",
        "        #copy the data to the numpy array\n",
        "        for j in range(batch_len):\n",
        "          cur_len = len(sentences[j])\n",
        "          if cur_len <= max_len:\n",
        "            batch_data[j][:cur_len] = sentences[j][:max_len]\n",
        "            batch_tags[j] = tags[j][:max_len]\n",
        "          else:\n",
        "            batch_data[j] = sentences[j][:max_len]\n",
        "            batch_tags[j] = tags[j][:max_len]\n",
        "\n",
        "        # since all data are indicies, we convert them to torch LongTensors\n",
        "        batch_data = torch.tensor(batch_data, dtype=torch.long)\n",
        "        batch_tags = torch.tensor(batch_tags, dtype=torch.long)\n",
        "\n",
        "        # shifts tensors to GPU if available\n",
        "        batch_data, batch_tags = batch_data.to(self.device), batch_tags.to(self.device)\n",
        "\n",
        "        yield batch_data, batch_tags\n",
        "\n",
        "      \n",
        "\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-aebf9c303338>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    def__init(self, data_dir, bert_model_dir, params, token_pad_idx=0):\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}