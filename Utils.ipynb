{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJolHzVeeRMe4oEwudclou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav-Tyro/keyphrase-extraction-using-BERT/blob/main/Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhWsk1A1WEtI"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVFuyjDOWizm"
      },
      "source": [
        "class Params():\n",
        "  \"\"\" Class that loads hyperparameters from json file.\n",
        "\n",
        "  Example:\n",
        "  '''\n",
        "  params = Params(json_path)\n",
        "  print(params.learning_rate)\n",
        "  params.learning_rate = 0.5 # change the value of learning_rate in params\n",
        "  '''\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, json_path):\n",
        "    with open(json_path) as f:\n",
        "      params = json.load(f)\n",
        "      self.__dict__.update(params)\n",
        "  def save(self, json_path):\n",
        "      \"\"\"Loads parameters from json file\"\"\"\n",
        "      with open(json_path) as f:\n",
        "        params = json.load(f)\n",
        "        self.__dict__.update(params)\n",
        "  \n",
        "  @property\n",
        "  def dict(self):\n",
        "    \"\"\" Gives dict_like access to params instance by 'params.dict['learning_rate']\"\"\"\n",
        "    return self.__dict__\n",
        "\n",
        "class RunningAverage():\n",
        "  \"\"\" A simple class that maintains the running average of a quantity\n",
        "\n",
        "  Example:\n",
        "  '''\n",
        "\n",
        "  loss_avg = RunningAverage()\n",
        "  loss_avg.update(2)\n",
        "  loaa_avg.update(4)\n",
        "  loss_avg() = 3\n",
        "  '''\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.steps = 0\n",
        "    self.total = 0\n",
        "\n",
        "  def update(self, val):\n",
        "    self.total += val\n",
        "    self.steps += 1\n",
        "  \n",
        "  def __call__(self):\n",
        "    return self.total / float(self.steps)\n",
        "\n",
        "def set_logger(log_path):\n",
        "  \"\"\"set the logger to log info in terminal and file 'log_path'.\n",
        "\n",
        "  In general , it is useful to have a logger so that every output to the terminal is saved\n",
        "  in  a permanent file. Here we save it to'model_dir/ train.log'.\n",
        "\n",
        "  Example:\n",
        "  '''\n",
        "  logging.info(\"starting training...\")\n",
        "  '''\n",
        "\n",
        "  Args:\n",
        "      log_path: (string) where to log\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  logger = logging.getLogger()\n",
        "  logger.setLevel(logging.INFO)\n",
        "\n",
        "  if not logger.handlers:\n",
        "    #logging to a file\n",
        "    file_handler = logging.FileHandler(log_path)\n",
        "    file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    #logging to console\n",
        "    stream_handler = logging.StramHandler()\n",
        "    stream_handler.setFormatter(logging.Formatter('%(messsage)s'))\n",
        "    logger.addHandler(stream_handler)\n",
        "\n",
        "def save_checkpoint(state,is_best, checkpoint):\n",
        "  \"\"\" Saves model and training parameters at checkopint + 'last.pth.tar'. If is_best== True, also saves checkpoint + 'best.pth.tar'\n",
        "\n",
        "\n",
        "  Args:\n",
        "      state: (dict) contains model's state_dict, may contain other keys such as opoch, optimizer sate_dict\n",
        "      is_best: (bool) True if it is the best model seen till now\n",
        "\n",
        "      checkpoint + 'best.pth.tar'\n",
        "  \"\"\"\n",
        "\n",
        "  filepath = os.path.join(checkpoint,'last.pth.tar')\n",
        "  if not os.path.exists(checkpoints):\n",
        "    print(\" checkpoint directory does not exist! Making directory {}\".format(checkpoint))\n",
        "    os.mkdir(checkpoint)\n",
        "  torch.save(sate, filepath)\n",
        "  if it_best:\n",
        "    shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer=None):\n",
        "  \"\"\"Loads mdoel parameters (state_dict) from file_path. if optimizer is provided , loads state_dict of\n",
        "  optimizer assuming  it is present in checkpoint.\n",
        "  \n",
        "\n",
        "  Args:\n",
        "      checkpoint: (string) filename which needs to be loaded\n",
        "      model: (torch.nn.module) model for which parameters are loaded\n",
        "      optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
        "  \"\"\"\n",
        "  if not os.path.exists(checkpoint):\n",
        "    raise (\"File doesn't exist {}\".format(checkpoint))\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "\n",
        "    #model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "      optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}