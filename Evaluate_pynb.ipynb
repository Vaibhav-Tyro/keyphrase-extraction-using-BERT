{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluate.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPq1qCzLLIhjOAkNDCqU08w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav-Tyro/keyphrase-extraction-using-BERT/blob/main/Evaluate_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IybSxSioEuLX",
        "outputId": "b74574b2-099c-4193-e415-23a20c32a62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "\"\"\" Evaluate the model\"\"\"\n",
        "%tb\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers  import BertForTokenClassification\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_dir', default='data/msra/', help = \"Directory containing the dataset\")\n",
        "parser.add_argument('--bert_model_dir', default = 'bert-base-chinese-pytorch', help = \"Directory containing the BERT model in PyTorch\")\n",
        "parser.add_argument('--model_dir', default = 'experiments/base_model', help = \"Directory containing params.json\")\n",
        "parser.add_argument('--seed', type = int, default = 23, help = \"random seed for initialization\")\n",
        "parser.add_argument('--restore_file', default = 'best', help = \"name of the file in 'model_dir' containing weights to load\")\n",
        "parser.add_argument('--multi_gpu', default = False, action='store_true', help = \"whether to use multiple GPUs if available\")\n",
        "\n",
        "def evaluate(model, data_iterator, params, marks='Eval', verbose = False):\n",
        "  \"\"\" Evaluate the model on 'steps' batches.\"\"\"\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  idx2tag = params.idx2tag\n",
        "\n",
        "  true_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  # a running average object for loss\n",
        "  loss_avg = utils.RunningAverage()\n",
        "\n",
        "  for _ in range(params.eval_steps):\n",
        "    #fetch the next evaluation batch\n",
        "    Batch_data, batch_tags = next(data_iterator)\n",
        "    batch_masks = batch_data.gt(0)\n",
        "\n",
        "    loss = model(batch_data, token_type_ids = None, attention_mask = batch_masks, labels = batch_tags)\n",
        "    if params.n_gpu > 1 and params.multi_gpu:\n",
        "      loss = loss.mean()\n",
        "    loss_avg.update(loss.item())\n",
        "\n",
        "    batch_output = model(batch_data, token_type_ids=None, attention_mask = batch_masks) #shape: (batch_size, max_len, num_labels)\n",
        "\n",
        "    batch_output = batch_output.detach().cpu().numpy()\n",
        "    batch_tags = batch_tags.to('cpu').numpy()\n",
        "\n",
        "    pred_tags.extend([idz2tag.get(idx) for indices in np.argmax(batch_output,axis = 2) for idx in indices])\n",
        "    true_tags.extend([idx2tag.get(idx) for indices in batch_tags for idx in indices])\n",
        "  assert len(pred_tags) == len(true_tags)\n",
        "\n",
        "  #logging loss, f1 and report\n",
        "  metrics = {}\n",
        "  f1 = f1_score(true_tags, pred_tags)\n",
        "  metrics['loss'] = loss_avg()\n",
        "  metrics['f1'] = f1\n",
        "  metrics_str = \"; \".join(\"{}: {05.2f}\".format(k,v) for k,v in metrics.items())\n",
        "  logging.info(\"-{} metrics: \".format(mark) + metrics_str)\n",
        "\n",
        "  if verbose:\n",
        "    report = classification_report(true_tags, pred_tags)\n",
        "    logging.info(report)\n",
        "  return metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  #Load the parameters from json file\n",
        "  json_path = os.path.join(args.model_dir, 'params.json')\n",
        "  assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
        "  params = utils.Params(json_path)\n",
        "\n",
        "  # Use  GPUs if available\n",
        "  params.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  params.n_gpu = torch.cuda.device_count()\n",
        "  params.multi_gpu = args.multi_gpu\n",
        "\n",
        "  # set the random seed for reproducible experiments\n",
        "  random.seed(args.seed)\n",
        "  torch.manual_seed(args.seed)\n",
        "  if params.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed) #set random seed for all GPUs\n",
        "  params.seed = args.seed\n",
        "\n",
        "  #set the logger\n",
        "  utils.set_logger(os.path.join(args.model_dir, 'evaluate.log'))\n",
        "\n",
        "  #create the input data pipeline\n",
        "  logging.info(\"loading the dataset...\")\n",
        "\n",
        "  #Initialize the DataLoader\n",
        "  data_loader = DataLoader(args.data_dir, args.bert_model_dir, params, token_pad_idx=0)\n",
        "\n",
        "  #Load data\n",
        "  test_data = data_loader.load_data('test')\n",
        "\n",
        "  #specify the test set size\n",
        "  params.test_size = test_data['size']\n",
        "  params.eval_steps = params.test_size // params.batch_size\n",
        "  test_data_iterator = data_loader.data_iterator(test_data, shuffle=False)\n",
        "\n",
        "  logging.info(\"-done.\")\n",
        "\n",
        "  #define the model\n",
        "  config_path = os.path.join(args.bert_model_dir, 'bert_config.json')\n",
        "  config = BertConfig.from_json_file(config_path)\n",
        "  model = BertForTokenClassification(config, num_labels = len(params.tag2idx))\n",
        "\n",
        "  model.to(params.device)\n",
        "  #Reload weights from the saved file\n",
        "  utils.load_checkpoint(os.path.join(args.model_dir, args.restore_file + '.pth.tar'), model)\n",
        "  if args.fp16:\n",
        "    model.half()\n",
        "  if params.n_gpu > 1 and args.multi_gpu:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "    logging.info(\"starting evaluation...\")\n",
        "    test_metrics = evaluate(model, test_data_iterator, params, mark='Test', verbose=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-58203891c321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;31m#Load the parameters from json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1746\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2402\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR]\n",
            "                             [--bert_model_dir BERT_MODEL_DIR]\n",
            "                             [--model_dir MODEL_DIR] [--seed SEED]\n",
            "                             [--restore_file RESTORE_FILE] [--multi_gpu]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-cf35deca-3b8c-42b2-a2a2-a244b916c1eb.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa4hhGE9Xifr",
        "outputId": "89943636-2f75-446c-a4eb-5596720d5cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "source": [
        "!pip install transformers\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c3306478ac59d23c4cdb8f0cb00c7c5d5dea65fb0a9850652fabcd860b029a7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}