{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPnCL+0kyH8+2pA/p4DoAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav-Tyro/keyphrase-extraction-using-BERT/blob/main/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcQnKFebbkpB"
      },
      "source": [
        "\"\"\"Train and evaluate the model\"\"\"\n",
        "import argparse\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn \n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from tqdm import trange\n",
        "from transformers import BertForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import utils"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVZpHFDQzu07"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_dir', default = 'data/task1', help = \"Directory containing the dataset\")\n",
        "parser.add_argument('--bert_model_dir', default='bert-base-uncased-pytorch', help = \"Directory containing the BERT model in pytorch\")\n",
        "parser.add_argument('--model_dir', default = 'experiment/base_model', help = \"Directory containing params.json\")\n",
        "parser.add_argument('--seed', type=int, default=2019, help = \"random seed for initialization\")\n",
        "parser.add_argument('--restore_file', default = None,\n",
        "                    help = \"optional, name of the file in --model_dir containing weights to reload before training\")\n",
        "parser.add_argument('--multi_gpu', default = False, action = 'store_true', help = \"whether to use multiple GPU if available\")\n",
        "parser.add_argument('--fp16', default = False, action = 'store_true', help = \"whether to use 16-bit float precission instead of 32-bit\")\n",
        "parser.add_argument('--loss_scale', type = float, default=0,\n",
        "                    help = \"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to true.\\n\"\n",
        "                    \"0 (default value):dynamic loss scaling.\\n\"\n",
        "                    \"positive power of 2: static loss scaling value.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VxiHtIH5LS-",
        "outputId": "0621a43e-02ae-4344-967f-6b319e94022b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "def train(model,data_iterator, optimizer, scheduler,params):\n",
        "  \"\"\" Train the model on 'steps' batches\"\"\"\n",
        "  #set model to training mode\n",
        "  model.train()\n",
        "  schedular.step()\n",
        "\n",
        "  #a running average object for loss\n",
        "  loss_avg = utils.RunningAverage()\n",
        "\n",
        "  #use tqdm for progress bar\n",
        "  t = trange(params.train_steps)\n",
        "  for i in t:\n",
        "    #fetch the next taining batch\n",
        "    batch_data, batch_tags = next(data_iterator)\n",
        "    batch_masks = batch_data.gt(0)\n",
        "\n",
        "    #compute model output and loss\n",
        "    loss = model(batch_data, token_type_ids = None, attention_mask = batch_masks, labels = batch_tags)\n",
        "\n",
        "    if params.n_gpu > 1 and args.multi_gpu:\n",
        "      loss = loss.mean() #mean() to average on multi-gpu\n",
        "\n",
        "      #clear previous  gradients, compute gradients of all variables with respect to loss\n",
        "      model.zero_grad()\n",
        "      if args.fp16:\n",
        "        optimizer.backward(loss)\n",
        "      else:\n",
        "          loss.backward()\n",
        "\n",
        "      # gradient clipping\n",
        "      nn.utils.clip_grad_norm_(parameters = model.parameters(), max_norm = params.clip_grad)\n",
        "\n",
        "      #performs update using calculated gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      #update the average loss\n",
        "      loss_avg.update(loss.item())\n",
        "      t.set_postfix(loss = '{:05.3f)'.format(loss_avg()))\n",
        "\n",
        "\n",
        "def train_and_evaluate(model,train_data, val_data, optimizer, scheduler, params, model_dir, restore_file= None):\n",
        "  \"\"\"Train the model and evaluate every epoch.\"\"\"\n",
        "\n",
        "  #reload weights from restore_file if specified\n",
        "\n",
        "  if restore_file is not None:\n",
        "    restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
        "    logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
        "    utils.load_checkpoint(restore_path, model, optimizer)\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    for epoch in range(1,params.epoch_num +1):\n",
        "      #Run one epoch\n",
        "      logging.info(\"Epoch {}/{}\".format(epoch, params.epoch_num))\n",
        "\n",
        "      #compute number of batches in one epoch\n",
        "      params.val_steps = params.train_size // params.batch_size\n",
        "      params.val_steps = params.val_size // params.batch_size\n",
        "\n",
        "      #data iterator for training\n",
        "      train_data_iterator = data_loader.data_iterator(train_data, shuffle=True)\n",
        "      #Train for one epoch on training set\n",
        "      train(model, train_data_iterator, optimizer, scheduler, params)\n",
        "\n",
        "      #data iterator for evaluation\n",
        "      train_data_iterator = data_loader.data_iterator(trainn_data, shuffle = False)\n",
        "      val_data_iterator = data_loader.data_iterator(val_data, shuffle = False)\n",
        "\n",
        "      #evaluate for one epoch on training set and validation set\n",
        "      params.eval_steps = params.train_steps\n",
        "      train_metrics = evaluate(model, train_data_iterator, params, mark-'Train')\n",
        "      params.eval_steps = params.val_steps\n",
        "      val_metrics = evaluate(model,val_data_iterator, params, mark='val')\n",
        "\n",
        "      val_f1 = val_metrics['f1']\n",
        "      improve_f1 = val_f1 - best_val_f1\n",
        "\n",
        "      #save weights of the network\n",
        "      model_to_save = model.module if hasattr(model, 'module') else model #only save the model it-self.\n",
        "      utils.save_checkpoint({'epoch': epoch + 1,\n",
        "                             'state_dict': model_to_save.state_dict(),\n",
        "                             'optim_dict': optimizer_to_save.state_dict()},\n",
        "                            is_best = improve_f1>0,\n",
        "                            checkpoints = model_dir)\n",
        "      \n",
        "      if improve_f1 > 0:\n",
        "        logging.info(\"-found new best F1\")\n",
        "        best_val_f1 = val_f1\n",
        "        if improve_f1 < params.patience:\n",
        "          patience_counter += 1\n",
        "        else:\n",
        "          patience_counter = 0\n",
        "      else:\n",
        "        patience_counter +=1\n",
        "\n",
        "\n",
        "        #Early stopping and logging best f1\n",
        "        if (patience_counter >= params.patience_num and epoch > params.min_epoch_num) or epoch == param.epoch_num:\n",
        "          logging.info(\"Best val f1: {:05.2f}\".format(best_val_f1))\n",
        "          break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  #Load the parameters from json file\n",
        "  json_path = os.path.join(args.model_dir, 'params.json')\n",
        "  assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
        "  params = utils.Params(json_path)\n",
        "\n",
        "  #Use GPUs if available\n",
        "  params.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  params.n_gpu = torch.cuda.device_count()\n",
        "  params.multi_gpu = args.multi_gpu\n",
        "\n",
        "  #set the random seed for reproducible experiments\n",
        "  random.seed(args.seed)\n",
        "  torch.manual_seed(args.seed)\n",
        "  if params.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed)  # set random seed for all GPUs\n",
        "  params.seed = args.seed\n",
        "\n",
        "  #set the logger\n",
        "  utils.set_logger(os.path.join(args.model_dir, 'train.log'))\n",
        "  logging.info(\"device: {}, n_gpu: {}, 16-bits training: {}\".format(params.device, params.n_gpu, args.fp16))\n",
        "\n",
        "  # Create the input data pipeline\n",
        "  logging.info(\"Loading the datasets...\")\n",
        "\n",
        "  #Initialize the Dataloader\n",
        "  data_loader = DataLoader(args.data_dir,args.bert_model_dir, params, token_pad_idx = 0)\n",
        "\n",
        "  # Load training data and test data\n",
        "  train_data = data_loader.load_data('train')\n",
        "  val_data = data_loader.load_data('val')\n",
        "\n",
        "  #Specify the training and validation dataset sizes\n",
        "  params.train_size = train_data['size']\n",
        "  params.val_size = val_data['size']\n",
        "\n",
        "  #Prepare model\n",
        "  model = BertTokenForClassification.from_pretrained(args.bert_model_dir, num_labels = len(params.tag2idx))\n",
        "  model.to(params.device)\n",
        "  if args.fp16:\n",
        "    model.half()\n",
        "\n",
        "  if params.n_gpu > 1 and args.multi_gpu:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "    #prepare optimizer\n",
        "    if params.full_finetuning:\n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = ['bias', 'gamma','beta']\n",
        "      optimizer_grouped_parameters = [\n",
        "                                      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                                       'weight_decay_rate': 0.001},\n",
        "                                      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                       'weight_decay_rate':0.0}\n",
        "      ]\n",
        "    else:\n",
        "      param_optimizer = list(model.classifier.named_parameters())\n",
        "      optimizer_grouped_parameters = [{'params': [p for n, p in param_optimzer]}]\n",
        "      if args.fp16:\n",
        "        try:\n",
        "          from apex.optimizers import FP16_Optimizer\n",
        "          from apex.optimizers import FusedAdam\n",
        "\n",
        "        except ImportError:\n",
        "          raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
        "                              lr=params.learning_rate,\n",
        "                              bias_correction=False,\n",
        "                              max_grad_norm=1.0)\n",
        "        scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 1/(1 + 0.05*opoch))\n",
        "        if args.loss_scale == 0:\n",
        "          optimizer = FP16_optimizer(optimizer, dynamic_loss_scale= True)\n",
        "        else:\n",
        "          optimizer = Fp16_optimizer(optimizer, static_loss_scale = args.loss_scale)\n",
        "      else:\n",
        "        optimizer = Adam(optimizer_grouped_parameters, lr = params.learning_rate)\n",
        "        scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1/(1 + 0.05*epoch))\n",
        "\n",
        "      #Train and evaluate the model\n",
        "      logging.info(\"starting training for {} epoch(s)\".format(params.epoch_num))\n",
        "      train_and_evaluate(model,train_data, val_data, optimizer, scheduler, params, args.model_dir, args.restore_file)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-35c88984e719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;31m#Load the parameters from json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-LhBpTaznQH",
        "outputId": "952a14b9-7497-4b5f-d1bc-3414a921d4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "pip install utils"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgHtO6kKzNm9",
        "outputId": "3e915296-e574-4496-aa52-a24e874888d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "pip install evaluate"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading https://files.pythonhosted.org/packages/90/50/0cc73b299fd941cb12d7ed39e0ccf8e18fe78dd6c16b951abe5477b3cd82/evaluate-0.0.3.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from evaluate) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from evaluate) (1.1.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from evaluate) (0.90)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->evaluate) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->evaluate) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->evaluate) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->evaluate) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->evaluate) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n",
            "Building wheels for collected packages: evaluate\n",
            "  Building wheel for evaluate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for evaluate: filename=evaluate-0.0.3-cp36-none-any.whl size=6862 sha256=3c0b71bd1e6280372a463f1577d4c0edd6f7a718c5d17aef654cbdf6597e6b24\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/51/a5/ebdce3e18b99539f31d3624ed21ca88ab3841617eb82628b05\n",
            "Successfully built evaluate\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMDebTFRyMoK",
        "outputId": "fdcd4748-f62a-4186-d1f3-c299a81d5e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fe42cd1dd109139dcb12730ea080938d3e4792a04b75b34d67a194836393d658\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}